{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdZmYJNgQNwM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import faiss\n",
        "import skdim\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skdim.id import TwoNN, ESS, MOM, KNN, DANCo, MiND_ML, MLE, lPCA\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Intrinsic Dimension calculation\n",
        "\n",
        "Data Points (sampled_features) - Known data points, that are used to compare against\n",
        "\n",
        "Query Points (y) - Compute LID for these points compared against data points (sampled_features) to find Nearest Neighbors and compute the LID based on these neighbors.\n",
        "\n",
        "### Basic Idea: Calculate LID for each query pt (y) using data points.\n",
        "\n",
        "---\n",
        "\n",
        "LID function approach:\n",
        "1. Add data points (sampled_features) to index\n",
        "2. For each query(y), compute distance to NN\n",
        "3. Compute LID for each query point(y)\n",
        "4. return average LID for all query points.\n",
        "\n",
        "---\n",
        "\n",
        "Core LID logic:\n",
        "1. Take maximum distance to NN\n",
        "2. Normalize all distances by maximum distance\n",
        "3. Take logs\n",
        "4. Average them\n"
      ],
      "metadata": {
        "id": "YbP8cvInQXYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Example\n",
        "\n",
        "Data Points (sampled_feats)\n",
        "*   A (1, 1)\n",
        "*   B (2, 2)\n",
        "* C (3, 3)\n",
        "* D (4, 4)\n",
        "* E (5, 5)\n",
        "\n",
        "Query Points (y)\n",
        "* Q1 (1.5, 1.5)\n",
        "* Q2 (4.5, 4.5)\n",
        "\n",
        "Compute LID for each query point y(Q1,Q2) against data points(A,B,C,D,E), return average of those 2 LIDs."
      ],
      "metadata": {
        "id": "4L3RUEVyTBIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_lid(y, sampled_feats, sample_size=-1, k_list=[200], metric='l2', block=50000):\n",
        "\n",
        "  # add data points to index\n",
        "    if metric == 'cos':\n",
        "        cpu_index = faiss.IndexFlatIP(sampled_feats.shape[1])\n",
        "        y = normalize(y)\n",
        "        sampled_feats = normalize(sampled_feats)\n",
        "    if metric == 'l2':\n",
        "        cpu_index = faiss.IndexFlatL2(sampled_feats.shape[1])\n",
        "\n",
        "    # print('cpu_index')\n",
        "    # gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n",
        "    cpu_index.add(np.ascontiguousarray(sampled_feats))\n",
        "\n",
        "    avg_lids = []\n",
        "\n",
        "  # compute distances to NN\n",
        "    for k in k_list:\n",
        "        i = 0\n",
        "        D = []\n",
        "        while i < y.shape[0]:\n",
        "           tmp = y[i:min(i + block, y.shape[0])]\n",
        "           i += block\n",
        "           b, nid = cpu_index.search(tmp, k)\n",
        "           b = np.sqrt(b)\n",
        "           D.append(b)\n",
        "\n",
        "  # Compute LID and return average\n",
        "        D = np.vstack(D)\n",
        "        # print(\"query finish\")\n",
        "        if metric == 'cos':\n",
        "          D = 1 - D  # cosine dist = 1 - cosine\n",
        "          D[D <= 0] = 1e-8\n",
        "        rk = np.max(D, axis=1)\n",
        "        rk[rk == 0] = 1e-8\n",
        "        lids = D / rk[:, None]\n",
        "        lids = -1 / np.mean(np.log(lids), axis=1)\n",
        "        lids[np.isinf(lids)] = y.shape[1]  # if inf, set as space dimension\n",
        "        lids = lids[~np.isnan(lids)]  # filter nan\n",
        "        avg_lids.append(lids.tolist())\n",
        "        # print('filter nan/inf shape', lids.shape)\n",
        "        # print('k', k - 1, 'lid_mean', np.mean(lids), 'lid_std', np.std(lids))\n",
        "    avg_lids = np.array(avg_lids).mean(axis=0)\n",
        "    return avg_lids"
      ],
      "metadata": {
        "id": "WFMjrqVbQUE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return AUROC for true labels and predicted score (average LID value), [0,1]\n",
        "def roc(corrects, scores):\n",
        "    auroc = metrics.roc_auc_score(corrects, scores)\n",
        "    return auroc"
      ],
      "metadata": {
        "id": "NLdooenUQUHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Logic\n",
        "### Basic Idea: Compute LID and AUROC for each layer separately"
      ],
      "metadata": {
        "id": "01ico38jVIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [i for i in range(15, 22)]\n",
        "p_value_for_layers = []\n",
        "\n",
        "name = 'tydiqa'\n",
        "name_map = {'tydiqa': 'Mistral-7B-v0.1_tydiqa', 'coqa': \"Mistral-7B-v0.1_coqa\", \"xsum\": \"Llama-2-7b-hf_xsum\"}\n",
        "\n",
        "\n",
        "\n",
        "for i in layers:\n",
        "  # Prediction tensors\n",
        "    pd = torch.load(os.path.join('./output_tensors', f\"{name_map[name]}_all_layer_{i}_pred.pt\"))\n",
        "\n",
        "  # extract the first prediction from each sample in pd and append it to pds\n",
        "    pds = []\n",
        "    num_samples = []\n",
        "    for k, v in pd.items():\n",
        "        pds.append(v[:1, :])\n",
        "        num_samples.append(v.shape[0])\n",
        "    pd = torch.cat(pds)\n",
        "\n",
        "  # Ground truth tensors\n",
        "    gt = torch.load(os.path.join('./output_tensors', f\"{name_map[name]}_all_layer_{i}_gt.pt\"))\n",
        "  # label tensors\n",
        "    labels = torch.load(os.path.join('./output_tensors', f\"{name_map[name]}_all_layer_1_label.pt\"))\n",
        "\n",
        "  # separate correct and incorrect predictions\n",
        "    true_pds = pd[labels == 1]\n",
        "    wrong_pds = pd[labels == 0]\n",
        "\n",
        "    # choose the first 500 examples as test\n",
        "    test_idxs = [i for i in range(gt.shape[0]) if i < 500]\n",
        "    train_idxs = [i for i in range(gt.shape[0]) if i not in test_idxs]\n",
        "\n",
        "    train_pd = gt[train_idxs, :].numpy().astype('float32')\n",
        "    train_labels = labels[train_idxs]\n",
        "\n",
        "\n",
        "    test_pd = pd[test_idxs, :]\n",
        "    test_gt = gt[test_idxs, :]\n",
        "    test_labels = labels[test_idxs]\n",
        "\n",
        "  # filter correct predictions from training data\n",
        "    correct_batch = []\n",
        "    for p, l in zip(train_pd, train_labels):\n",
        "        if l.item() == 1:\n",
        "            correct_batch.append(p.tolist())\n",
        "    correct_batch = np.array(correct_batch).astype('float32')\n",
        "\n",
        "\n",
        "  # compute LID and AUROC\n",
        "    numbers = correct_batch.shape[0]\n",
        "    k_list = [numbers - 1]\n",
        "    for k in k_list:\n",
        "        lids = compute_lid(test_pd.numpy(), correct_batch, sample_size=-1, k_list=[k], metric='l2', block=50000)\n",
        "        # gt_lids = compute_lid(test_gt.numpy(), correct_batch, sample_size=-1, k_list=[k], metric='l2', block=50000)\n",
        "        auroc = roc(test_labels, -lids)"
      ],
      "metadata": {
        "id": "-FkQuYvhQUJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems:\n",
        "#### 1. The approach only calculates LID and AUROC scores for specific layer, hallucination detection logic missing\n",
        "#### 2. Layer Selection Logic not implemented: We will have N LID & AUROC values for N layers.\n",
        "#### 3. Thresholding logic not implemented: How do we select the LID value of truthfull answer?\n",
        "#### 4. Incorrect Loop Logic in LID and AUROC Calculation: Final LID and AUROC values only consider the last value of k due to incorrect loop logic, leading to potentially incomplete evaluation results."
      ],
      "metadata": {
        "id": "oyT3Z7dBV9_8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJBtPWh3QUO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}